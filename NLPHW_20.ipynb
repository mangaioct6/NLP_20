{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bbb039",
   "metadata": {},
   "source": [
    "## NLP_HW_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2456c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f32166",
   "metadata": {},
   "source": [
    "## 3.Load the tokenized Paradise Lost from the Gutenberg Corpus in NLTK.* https://www.nltk.org/book/ch02.html . Stem or lemmatize the words and find counts. Select the top 20 words and create a histogram. Exclude stop words and make sure you are including words of all capitalizations in your count. If there are any meaningless “words” (“thus” and single letters, etc.) that are produced in your list or top words, alter your logic to exclude them. Specify why you chose stemming or lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9b8f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gutenberg corpus\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8ac96bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Paradise Lost sentences \n",
    "paradise_sent = nltk.corpus.gutenberg.sents('milton-paradise.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4459a1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', ...]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading paradise lost words\n",
    "paradise_words = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "paradise_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff8ab5",
   "metadata": {},
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2844bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stop words. NOTE: all stop words are in lower case\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fee97",
   "metadata": {},
   "source": [
    "* After we print few sentences, we can do some text preprocessing. Lets remove stop words from paradise first few  sentences  and then we can use lemmetizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893f915",
   "metadata": {},
   "source": [
    "#### Converting into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aece2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using list comprehension(LC) to lower the words from lost paadise words\n",
    "lower_case = [i.lower() for i in paradise_words ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62641c89",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "NOTE: from sentences we cant remove stop words, either we have to convert it into words(list) or we can use word tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7086884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_stopword = [j for j in lower_case if j not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfe0c5",
   "metadata": {},
   "source": [
    "#### Using lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b97a0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "14fe931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 10198), (';', 2317), ('.', 1254), (':', 748), (\"'\", 595), ('heaven', 436), ('thou', 432), ('thy', 414), ('thee', 358), ('-', 334), ('thus', 318), ('god', 316), ('?', 301), ('shall', 283), ('!', 251), ('yet', 228), ('earth', 217), ('though', 217), ('u', 187), ('man', 176)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# creating an object for lemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# using list comprehension applying lemmatizer to the stop words removed sentence\n",
    "lemma_sent = [lemma.lemmatize(i) for i in rem_stopword]\n",
    "\n",
    "# counting bag of words using counter\n",
    "from collections import Counter\n",
    "bag_words = Counter(lemma_sent)\n",
    "\n",
    "# printing 20 words which are the most commom\n",
    "print(bag_words.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e31fc",
   "metadata": {},
   "source": [
    "#### Using Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ba01c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 10198), (';', 2317), ('.', 1254), (':', 748), (\"'\", 595), ('heaven', 485), ('thou', 432), ('thi', 414), ('thee', 358), ('-', 334), ('thus', 318), ('god', 316), ('?', 301), ('shall', 283), ('!', 251), ('earth', 228), ('yet', 228), ('though', 217), ('us', 187), ('man', 178)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Instatntiating stemmer\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "\n",
    "# using stemmer in lost paradise\n",
    "stemmed_paradise = [stemmer.stem(i) for i in rem_stopword]\n",
    "stemmed_paradise\n",
    "\n",
    "# already imported counter\n",
    "bag_stem = Counter(stemmed_paradise)\n",
    "print(bag_stem.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1567e0c",
   "metadata": {},
   "source": [
    "#### Specify why you chose stemming or lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b7a10",
   "metadata": {},
   "source": [
    "* From both of my lemmatizer and stemmer results, I decided to use lemmetizer. Because in stemmer some words aren't stemmed meaningfully. For instance, **('forc', 7)** force stemmed forc,**('chang', 6)** change as chang. But lemmatizer gave full meaning full words. **Hence I chose lemmatizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a9e694",
   "metadata": {},
   "source": [
    "* my most common words contains punctuations and some meaningless words like st, th. I am enhancing my preprocseeing by  removing punctuations and again see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "421024ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In python we have a builtin function is_alnum() used to print only alphatetical letters and numbers if its have.\n",
    "# To remove punctuations I am using that function in LC\n",
    "\n",
    "alpha_only = [i for i in lemma_sent if i.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1912bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words contain some meaning less words like th, u. I am removing those.\n",
    "meaningless = ['th','u','thus','though','thou','yet','thee','thy']\n",
    "\n",
    "# removing these words from our alphanumeric only list\n",
    "final_words = [i for i in alpha_only if i not in meaningless]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b171e8",
   "metadata": {},
   "source": [
    "#### Now punctuations are removed, checking bag counts using lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ac5bda7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Heaven', 436), ('God', 316), ('Shall', 283), ('Earth', 217), ('Man', 176), ('First', 175), ('Day', 157), ('High', 153), ('One', 142), ('Son', 132), ('Far', 129), ('Death', 127), ('May', 126), ('Power', 125), ('Great', 123), ('Till', 120), ('Like', 120), ('World', 119), ('Hell', 119), ('Thing', 116)]\n"
     ]
    }
   ],
   "source": [
    "lemma_sent = [lemma.lemmatize(i) for i in final_words]\n",
    "lemma_sent = [i.capitalize() for i in lemma_sent]\n",
    "\n",
    "# counting bag of words using counter\n",
    "bag_words = Counter(lemma_sent)\n",
    "\n",
    "# printing 20 words which are the most commom\n",
    "print(bag_words.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88788f3",
   "metadata": {},
   "source": [
    "#### Unpacking tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c0341c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Heaven', 'God', 'Shall', 'Earth', 'Man', 'First', 'Day', 'High', 'One', 'Son', 'Far', 'Death', 'May', 'Power', 'Great', 'Till', 'Like', 'World', 'Hell', 'Thing')\n",
      "(436, 316, 283, 217, 176, 175, 157, 153, 142, 132, 129, 127, 126, 125, 123, 120, 120, 119, 119, 116)\n"
     ]
    }
   ],
   "source": [
    "# my bag of words is a list of tuples. I need words and word count separately to plot histogram. Unzipping tuple will \n",
    "# give those separate values. I can convert this into dataframe and plot histogram or I can unpack the tuples.\n",
    "\n",
    "bow_20 = bag_words.most_common(20)\n",
    "words, count = zip(*bow_20)\n",
    "print(words)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407b20d",
   "metadata": {},
   "source": [
    "#### Plotting bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e2a265a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(words,count)\n",
    "plt.xlabel(\"First 20 Most Common words\")\n",
    "plt.ylabel(\"No. of Count\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0b580",
   "metadata": {},
   "source": [
    "## 4.Perform Vader Sentiment Analysis on the book. Find the 5 most negative, 5 most positive, and 5 most neutral sentences in Paradise Lost. http://www.nltk.org/howto/sentiment.html . This may take a while to run, so you can always start with a small subset of the data (100 sentences) and then once your code works as expected, expand it to the whole book and let it run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f179513",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "* Sentiment analysis is a text analysis method that detects polarity(i.e., positve or negatice opinion) within the text or paragraph or sentence.\n",
    "\n",
    "* VADAR(Valence Aware Dictionary for sEntiment Reasoning) is a text analysis method that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58223237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print few sentences from paradise lost\n",
    "paradise_sentence_10 = nltk.corpus.gutenberg.sents('milton-paradise.txt')[:10]\n",
    "#paradise_sentence_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b84bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading entire book\n",
    "paradise_sentence = nltk.corpus.gutenberg.sents('milton-paradise.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281c0d7",
   "metadata": {},
   "source": [
    "#### Removing punctuations and spaces from the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f0af49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Paradise Lost by John Milton 1667 ]Book IOf Man  s first disobedience,and the fruit Of that forbidden tree whose mortal taste Brought death into the World,and all our woe,With loss of Eden,till one greater Man Restore us,and regain the blissful seat,Sing,Heavenly Muse,that,on the secret top Of Oreb,or of Sinai,didst inspire That shepherd who first taught the chosen seed In the beginning how the heavens and earth Rose out of Chaos:or,if Sion hill Delight thee more,and Siloa  s brook that flowed Fast by the oracle of God,I thence Invoke thy aid to my adventurous song,That with no middle flight intends to soar Above th  Aonian mount,while it pursues Things unattempted yet in prose or rhyme.And chiefly thou,O Spirit,that dost prefer Before all temples th  upright heart and pure,Instruct me,for thou know  st;thou from the first Wast present,and,with mighty wings outspread,Dove - like sat  st brooding on the vast Abyss,And mad  st it pregnant:what in me is dark Illumine,what is low raise and support;That,to the height of this great argument,I may assert Eternal Providence,And justify the ways of God to men.Say first--for Heaven hides nothing from thy view,Nor the deep tract of Hell--say first what cause Moved our grand parents,in that happy state,Favoured of Heaven so highly,to fall off From their Creator,and transgress his will For one restraint,lords of the World besides.Who first seduced them to that foul revolt?Th  infernal Serpent;he it was whose guile,Stirred up with envy and revenge,deceived The mother of mankind,what time his pride Had cast him out from Heaven,with all his host Of rebel Angels,by whose aid,aspiring To set himself in glory above his peers,He trusted to have equalled the Most High,If he opposed,and with ambitious aim Against the throne and monarchy of God,Raised impious war in Heaven and battle proud,With vain attempt.Him the Almighty Power Hurled headlong flaming from th  ethereal sky,With hideous ruin and combustion,down To bottomless perdition,there to dwell In adamantine chains and penal fire,Who durst defy th  Omnipotent to arms.Nine times the space that measures day and night To mortal men,he,with his horrid crew,Lay vanquished,rolling in the fiery gulf,Confounded,though immortal.But his doom Reserved him to more wrath;for now the thought Both of lost happiness and lasting pain Torments him:round he throws his baleful eyes,That witnessed huge affliction and dismay,Mixed with obdurate pride and steadfast hate.\n"
     ]
    }
   ],
   "source": [
    "def plain_sentence(sentenc):\n",
    "    sent = ' '.join(sentenc).replace(' , ',',').replace(' .','.').replace(' ? ','?').replace(' : ',':')\\\n",
    "    .replace('  ','').replace(' )',')').replace(' !','!').replace(' ?','?').replace('\\'','').replace(' ,',',')\\\n",
    "    .replace(' .','.').replace(' ; ',';').replace(' ;',';').replace(' -- ','--').replace(' :',':').replace(' \" ','\"')\n",
    "    return sent\n",
    "    \n",
    "\n",
    "plain_sent = \"\"\n",
    "for i in range(0,len(paradise_sentence_10)):\n",
    "    plain_sent = plain_sent + plain_sentence(paradise_sentence_10[i])\n",
    "\n",
    "print(plain_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ef901",
   "metadata": {},
   "source": [
    "#### Calling the function for entire book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "090c2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_sent = \"\"\n",
    "for i in range(0,len(paradise_sentence)):\n",
    "    plain_sent = plain_sent + plain_sentence(paradise_sentence[i])\n",
    "    \n",
    "#print(plain_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d29034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Codergirl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c6ed3",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis for entire book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14f2b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'neg': 0.1, 'neu': 0.746, 'pos': 0.154, 'compound': 1.0}]\n",
      "Paradise Lost book was rated as  10.0 % Negative\n",
      "Paradise Lost book was rated as  15.4 % Positive\n",
      "Paradise Lost book was rated as  74.6 % Neutral\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# creating an object for sentiment analyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# finding polarity\n",
    "sentiment = sentiment_analyzer.polarity_scores(plain_sent)\n",
    "\n",
    "print(sentiment_list)\n",
    "print(\"Paradise Lost book was rated as \",sentiment['neg']*100, \"% Negative\")\n",
    "print(\"Paradise Lost book was rated as \",sentiment['pos']*100, \"% Positive\")\n",
    "print(\"Paradise Lost book was rated as \",sentiment['neu']*100, \"% Neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796ba86",
   "metadata": {},
   "source": [
    "* Since I have to print most positive, negative and neutral lines, I am finding polarity to every lines in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "384798cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plain_sentlist =[]\n",
    "for i in range(0,len(paradise_sentence)):\n",
    "    plain_sentlist.append(plain_sentence(paradise_sentence[i]))\n",
    "    \n",
    "sentiment_list = []\n",
    "for i in range(0,len(plain_sentlist)):\n",
    "    sentiment = sentiment_analyzer.polarity_scores(plain_sentlist[i]) \n",
    "    sentiment_list.append({\"Positive\":sentiment['pos'],\n",
    "            \"Negative\":sentiment['neg'],\n",
    "            \"Neutral\":sentiment['neu']})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2770e",
   "metadata": {},
   "source": [
    "* I am converting sentiment analysis of entire lines into dataframe. So that I can print the top 5 list from descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f47f56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.141</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.256</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>0.323</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>0.096</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1851 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Positive  Negative  Neutral\n",
       "0        0.400     0.219    0.381\n",
       "1        0.000     0.000    1.000\n",
       "2        0.181     0.088    0.731\n",
       "3        0.141     0.071    0.788\n",
       "4        0.256     0.033    0.712\n",
       "...        ...       ...      ...\n",
       "1846     0.323     0.097    0.581\n",
       "1847     0.091     0.000    0.909\n",
       "1848     0.091     0.033    0.876\n",
       "1849     0.096     0.112    0.792\n",
       "1850     0.000     0.000    1.000\n",
       "\n",
       "[1851 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_list)\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f667373",
   "metadata": {},
   "source": [
    "* Appended sentences into the dataframe to print the most 5 positive, negative, neutral lines from the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fa01d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.381</td>\n",
       "      <td>[ Paradise Lost by John Milton 1667 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Book I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.731</td>\n",
       "      <td>Of Man  s first disobedience,and the fruit Of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.141</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.788</td>\n",
       "      <td>And chiefly thou,O Spirit,that dost prefer Bef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.256</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.712</td>\n",
       "      <td>Say first--for Heaven hides nothing from thy v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>0.323</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.581</td>\n",
       "      <td>This further consolation yet secure I carry he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.909</td>\n",
       "      <td>So spake our mother Eve;and Adam heard Well pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.876</td>\n",
       "      <td>High in front advanced,The brandished sword of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>0.096</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.792</td>\n",
       "      <td>They,looking back,all the eastern side beheld ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[ The End ]\u001a\u001a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1851 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Positive  Negative  Neutral  \\\n",
       "0        0.400     0.219    0.381   \n",
       "1        0.000     0.000    1.000   \n",
       "2        0.181     0.088    0.731   \n",
       "3        0.141     0.071    0.788   \n",
       "4        0.256     0.033    0.712   \n",
       "...        ...       ...      ...   \n",
       "1846     0.323     0.097    0.581   \n",
       "1847     0.091     0.000    0.909   \n",
       "1848     0.091     0.033    0.876   \n",
       "1849     0.096     0.112    0.792   \n",
       "1850     0.000     0.000    1.000   \n",
       "\n",
       "                                              sentences  \n",
       "0                 [ Paradise Lost by John Milton 1667 ]  \n",
       "1                                                Book I  \n",
       "2     Of Man  s first disobedience,and the fruit Of ...  \n",
       "3     And chiefly thou,O Spirit,that dost prefer Bef...  \n",
       "4     Say first--for Heaven hides nothing from thy v...  \n",
       "...                                                 ...  \n",
       "1846  This further consolation yet secure I carry he...  \n",
       "1847  So spake our mother Eve;and Adam heard Well pl...  \n",
       "1848  High in front advanced,The brandished sword of...  \n",
       "1849  They,looking back,all the eastern side beheld ...  \n",
       "1850                                      [ The End ]\u001a\u001a  \n",
       "\n",
       "[1851 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df['sentences'] = plain_sentlist\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a2e75",
   "metadata": {},
   "source": [
    "#### Most 5 Positive sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d656d8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>1.000</td>\n",
       "      <td>O Heaven!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1.000</td>\n",
       "      <td>O Heaven!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>1.000</td>\n",
       "      <td>O friends!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>1.000</td>\n",
       "      <td>O Friends!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0.787</td>\n",
       "      <td>Courageous Chief!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Positive          sentences\n",
       "828      1.000          O Heaven!\n",
       "1441     1.000          O Heaven!\n",
       "605      1.000         O friends!\n",
       "908      1.000         O Friends!\n",
       "626      0.787  Courageous Chief!"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positve_df = sentiment_df[['Positive','sentences']]\n",
    "most_5_pos = positve_df.sort_values(ascending = False, by ='Positive')\n",
    "most_5_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d4e95",
   "metadata": {},
   "source": [
    "#### Most 5 Negative sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfa071f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>1.000</td>\n",
       "      <td>O Hell!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.000</td>\n",
       "      <td>No!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>1.000</td>\n",
       "      <td>Alas!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.777</td>\n",
       "      <td>Me miserable!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.773</td>\n",
       "      <td>I fled,and cried out Death!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Negative                    sentences\n",
       "512      1.000                      O Hell!\n",
       "153      1.000                          No!\n",
       "1689     1.000                        Alas!\n",
       "478      0.777                Me miserable!\n",
       "314      0.773  I fled,and cried out Death!"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_df = sentiment_df[['Negative','sentences']]\n",
    "most_5_neg =negative_df.sort_values(ascending = False, by ='Negative').head()\n",
    "most_5_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5d4d1",
   "metadata": {},
   "source": [
    "#### Most 5 Neutral sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0e85997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neutral</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[ The End ]\u001a\u001a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Oft in her absence mimick Fancy wakes To imita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>1.0</td>\n",
       "      <td>To whom the Tempter guilefully replied.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Here,happy creature,fair angelick Eve!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The one seemed woman to the waist,and fair,But...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Neutral                                          sentences\n",
       "1850      1.0                                      [ The End ]\u001a\u001a\n",
       "673       1.0  Oft in her absence mimick Fancy wakes To imita...\n",
       "1274      1.0            To whom the Tempter guilefully replied.\n",
       "665       1.0             Here,happy creature,fair angelick Eve!\n",
       "284       1.0  The one seemed woman to the waist,and fair,But..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Neutral_df = sentiment_df[['Neutral','sentences']]\n",
    "most_5_neu = Neutral_df.sort_values(ascending = False, by ='Neutral').head()\n",
    "most_5_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fdae7dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The one seemed woman to the waist,and fair,But ended foul in many a scaly fold,Voluminous and vast--a serpent armed With mortal sting.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df['sentences'][284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5612e910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oft in her absence mimick Fancy wakes To imitate her;but,misjoining shapes,Wild work produces oft,and most in dreams;Ill matching words and deeds long past or late.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df['sentences'][673]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28651185",
   "metadata": {},
   "source": [
    "## 5. Explain your findings from the previous question. Are the sentences and their sentiment analysis scores correct? Explain why or why not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d975b6d",
   "metadata": {},
   "source": [
    "* All my top 5 positive sentences looks delightful. It is expressing happiness. So I agree with my positive findings.\n",
    "\n",
    "* If you see my top 5 negative sentences(**OHell, No!!, Alas!! , miserable, cried**) are expressing sadness. So sentiment analysis with negative commands are also acceptable.\n",
    "\n",
    "* All 4 sentences other that **\"Here,happy creature,fair angelick Eve!\"** sentences from neutral seems to be without any expressions. But I doubt this one. My analysis shows its 100% neutral. My thoughts about is Happy creature, eves might come under positive sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2207bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
